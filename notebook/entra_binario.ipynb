{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asfRC48B3cr8"
      },
      "source": [
        "En este notebook se hara el proceso de **PRIORIZAR** , se tomaran las PQR Desde diciembre del 2022 a  **2023 a Marzo(periodo de tres meses)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFjCIGvRLWHN"
      },
      "source": [
        "# CONFIGURAR SPARK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.append('../librerias')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSySf-J3Oiy"
      },
      "source": [
        "# LIBRERIAS\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v3kkbvVU9pJ"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "!python -m spacy download es_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HQpIbqTHV86",
        "outputId": "21ef21bc-f152-438d-a8bb-986ebedcc8c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "import gc\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import os\n",
        "import re, string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "#from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re, string\n",
        "\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer, util\n",
        "#from transformers import *\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', disable=['tagger', 'parser', 'ner'])\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#.master(\"local[2]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pycorrector\n",
        "%pip install unidecode\n",
        "%pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26MilXbg3TbA"
      },
      "source": [
        "# ETL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uf-CJREU8cSq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cbd41468-b0f3-4d3c-868b-b215cda33435\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>FECHA_CREACION</th>\n",
              "      <th>NUMERO_PQR</th>\n",
              "      <th>DESCRIPCION_PQR</th>\n",
              "      <th>clasificacion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>22/02/23 9:19</td>\n",
              "      <td>MDM-PQR-37253010</td>\n",
              "      <td>ZULEIMA ALEZANDRA TORRES DIAZ  1031134671 CC  ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>14/02/23 18:20</td>\n",
              "      <td>MDM-PQR-37091363</td>\n",
              "      <td>ZAIDA GUITIERREZ  52325972 CC 6013640557  3209...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3/12/22 8:51</td>\n",
              "      <td>MDM-PQR-35476275</td>\n",
              "      <td>YURY SUAREZ LLANES  51978002 CC  6012169386  3...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>14/02/23 17:18</td>\n",
              "      <td>MDM-PQR-37089931</td>\n",
              "      <td>YULY MORENO  52381328 CC  6012529406  31678196...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>14/02/23 21:44</td>\n",
              "      <td>MDM-PQR-37094750</td>\n",
              "      <td>YULY ANDREA GOMEZ ROMERO  1015467573 CC 314344...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15740</th>\n",
              "      <td>15740</td>\n",
              "      <td>21/02/23 17:11</td>\n",
              "      <td>MDM-PQR-37240623</td>\n",
              "      <td>LINEA NOMBRE JULY ROCIO RIANO RIANO   CED...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15741</th>\n",
              "      <td>15741</td>\n",
              "      <td>10/01/23 13:01</td>\n",
              "      <td>MDM-PQR-36207108</td>\n",
              "      <td>NOMBRE:  YENNY  CEDULA:   51760212 LINE...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15742</th>\n",
              "      <td>15742</td>\n",
              "      <td>15/12/22 11:42</td>\n",
              "      <td>MDM-PQR-35702774</td>\n",
              "      <td>NOMBRE:    EMILSEN CALDERON GARZON   C...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15743</th>\n",
              "      <td>15743</td>\n",
              "      <td>25/02/23 15:29</td>\n",
              "      <td>MDM-PQR-37337458</td>\n",
              "      <td>\"Línea: Nombre de quien se comunica: ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15744</th>\n",
              "      <td>15744</td>\n",
              "      <td>6/12/22 16:02</td>\n",
              "      <td>MDM-PQR-35546962</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15745 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbd41468-b0f3-4d3c-868b-b215cda33435')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cbd41468-b0f3-4d3c-868b-b215cda33435 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cbd41468-b0f3-4d3c-868b-b215cda33435');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       index  FECHA_CREACION        NUMERO_PQR  \\\n",
              "0          0   22/02/23 9:19  MDM-PQR-37253010   \n",
              "1          1  14/02/23 18:20  MDM-PQR-37091363   \n",
              "2          2    3/12/22 8:51  MDM-PQR-35476275   \n",
              "3          3  14/02/23 17:18  MDM-PQR-37089931   \n",
              "4          4  14/02/23 21:44  MDM-PQR-37094750   \n",
              "...      ...             ...               ...   \n",
              "15740  15740  21/02/23 17:11  MDM-PQR-37240623   \n",
              "15741  15741  10/01/23 13:01  MDM-PQR-36207108   \n",
              "15742  15742  15/12/22 11:42  MDM-PQR-35702774   \n",
              "15743  15743  25/02/23 15:29  MDM-PQR-37337458   \n",
              "15744  15744   6/12/22 16:02  MDM-PQR-35546962   \n",
              "\n",
              "                                         DESCRIPCION_PQR clasificacion  \n",
              "0      ZULEIMA ALEZANDRA TORRES DIAZ  1031134671 CC  ...           1.0  \n",
              "1      ZAIDA GUITIERREZ  52325972 CC 6013640557  3209...           0.0  \n",
              "2      YURY SUAREZ LLANES  51978002 CC  6012169386  3...           1.0  \n",
              "3      YULY MORENO  52381328 CC  6012529406  31678196...           0.0  \n",
              "4      YULY ANDREA GOMEZ ROMERO  1015467573 CC 314344...           0.0  \n",
              "...                                                  ...           ...  \n",
              "15740       LINEA NOMBRE JULY ROCIO RIANO RIANO   CED...           1.0  \n",
              "15741         NOMBRE:  YENNY  CEDULA:   51760212 LINE...           0.0  \n",
              "15742          NOMBRE:    EMILSEN CALDERON GARZON   C...           1.0  \n",
              "15743           \"Línea: Nombre de quien se comunica: ...           1.0  \n",
              "15744                                                  0           0.0  \n",
              "\n",
              "[15745 rows x 5 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "pandas_c_F= pd.read_csv('/content/PRIORIZAR/entranamiento_/binario_entrenamiento_cruda.csv',delimiter=';')\n",
        "\n",
        "pandas_c_F= pandas_c_F.fillna(\"vacios\")\n",
        "\n",
        "\n",
        "pandas_c_F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "index              0\n",
              "FECHA_CREACION     0\n",
              "NUMERO_PQR         0\n",
              "DESCRIPCION_PQR    0\n",
              "clasificacion      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pandas_c_F.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entro2\n"
          ]
        }
      ],
      "source": [
        "from clean_text import clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'usuairo queire valdidar  el saldo pendiente'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#declare answers and questions\n",
        "descripcion=pandas_c_F.DESCRIPCION_PQR\t\n",
        "\n",
        "\n",
        "\n",
        "# Cleaning the questions\n",
        "\n",
        "clean_descripcion = []\n",
        "for des in descripcion:\n",
        "    clean_descripcion.append(clean_text(des))\n",
        "\n",
        "\n",
        "#restar dos para optener la data de excel y poder comparar\n",
        "\n",
        "clean_descripcion  [1002]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_descripcion  [15744]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from spelling import correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 15745/15745 [50:19<00:00,  5.21it/s]  \n"
          ]
        }
      ],
      "source": [
        "corrected_sentences = []\n",
        "for text in tqdm(clean_descripcion, desc=\"Processing\"):\n",
        "    corrected_sentence = correct_sentence(text)\n",
        "    corrected_sentences.append(corrected_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "corrected_sentences1=corrected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'validar facturas en mora se informa factura emitida ya tiene conceptos en mora pagar en su totalidad para la reconeporion del servicio'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrected_sentences1  [933]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['algo', 'os', 'es', 'hubiesen', 'seq', 'seríamos', 'ud', 'otro', 'habría', 'time', 'uno', 'esté', 'seréis', 'mías', 'demá', 'estarás', 'tenemos', 'tuyo', '\\\\', 'tuviéramos', 'feb', 'etb', 'int', 'sentidas', 'habréis', 'mdmpqr', 'estabais', 'eran', 'qué', 'teniendo', 'hubieran', 'sintiendo', 'abril', 'habías', 'esto', 'solo', 'luego', 'estuvieras', 'hubieron', 'has', 'aaron', 'tuviese', 'esos', 'este', 'mayo', 'min', 'haya', 'habrá', '@hotmail', 'saawz', 'estas', 'tengan', 'cel', 'cliente', 'estado', 'cl', 'unos', 'bytes', 'menjura', 'tienes', 'suyo', 'ipc', '‘', 'crm', 'había', 'tenida', 'tenéis', 'sra', '*', 'hayáis', 'estuvieran', '`', 'tendrías', 'estemos', 'su', 'tenía', 'mayerli', 'esa', 'tuyas', 'tenidas', 'las', 'él', 'estuvierais', 'otalvaro', 'tuvo', 'ciudadbogota', 'nos', 'fueras', 'suya', '\"\"', 'estaríamos', 'estadas', '%', 'tuviesen', '?', 'estamos', 'hubieseis', 'vuestra', 'tenidos', 'documento', 'enero', 'chat', 'max', 'otros', 'estuvimos', 'algunos', 'habríamos', 'fueran', 'habéis', 'tus', 'estás', 'estuvieseis', 'tengo', 'carmona', 'estuvieses', 'dirección', 'estuviésemos', 'esta', 'hayas', '(', 'bis', 'en', 'ss', 'también', 'estuvo', 'tuvieron', 'estados', 'email', 'son', '[', 'junio', 'mucho', 'estábamos', 'del', 'mogollon', 'desde', 'edwin', 'de', 'todos', 'hube', 'cual', '“', '``', 'algunas', 'éramos', 'línea', '...', 'tendremos', 'nosotras', '^', '!', 'hotmail', 'fueses', 'ok', 'sí', 'ellos', ')', 'dirar', 'tanto', 'tengamos', 'alejandro', 'atar', 'cédula', 'la', 'tuvimos', 'sois', '.', 'muchos', 'tendréis', 'estarías', 'estabas', 'tenían', 'p', 'tendríamos', 'jose', 'habido', 'comunica', 'sentido', 'pablo', 'como', '&', 'jorge', ',', 'tenías', 'cortes', 'estáis', 'estada', 'tendrían', 'tienen', 'estaría', 'juan', 'soy', 'sterns', 'habremos', 'tendrá', 'app', 'diciembreap', 'estuviese', 'con', 'antes', '}', 'Gracias', 'se', 'estarían', 'hubieses', 'estar', 'tuviste', 'los', 'que', 'quien', 'hubiese', 'mis', 'Reportó', 'será', 'sean', 'habían', 'in', 'una', 'sur', 'ms', '-', 'tendrás', 'agosto', 'tuvisteis', '_', 'ion', '—', 'siente', 'sur in', 'com', 'habíais', 'están', 'octubre', 'habríais', 'seremos', 'eso', 'nn', '>', 'correo', 'marzo', 'fuera', 'avg', 'diciembre', 'nada', 'estad', 'fueseis', 'tengáis', 'nuestras', 'teníamos', 'vuestros', 'atención', 'sobre', 'nuestros', 'tuvieras', 'usuario', 'b', 'fuisteis', 'what', 'vuestro', 'ref', 'estarán', 'aclara', 'estéis', 'estuviste', 'cc', 'ciudadania', 'gracias', 'whastapp', 'suyos', 'nosotros', 'sus', 'otras', 'e', 'yohana poveda', 'estaban', '/', 'suyas', 'serán', ';', 'facebook', 'esas', 'tec', 'estaríais', 'sería', 'tengas', 'estén', 'ct', 'vosotros', 'reply', 'hubiera', 'fuese', '|', 'habidos', 'tened', 'hubierais', 'ellas', 'megas', 'durante', 'departamentobogota', 'seríais', ':', 'from', 'habida', '--', 'sido', 'fuesen', 'yo', 'hra', 'estuviesen', 'fuerais', 'estos', 'somos', 'estaréis', 'última', 'bermuda', 'habidas', '@', 'tuyos', 'a', 'vuestras', 'seas', 'estuve', 'angela', 'habíamos', 'fuéramos', 'fuimos', 'sea', 'hasta', 'fuiste', 'más', 'tú', 'seamos', 'tenido', 'habiendo', 'número', 'ftth', 'id', 'whatsapp', 'tuvieseis', 'tendrán', 'tu', 'comunicación', 'hubiéramos', 'estuviera', 'Descripcion', 'hubiésemos', 'gmail', 'al', 'mí', 'manifiesta', 'habrían', 'tenga', 'pero', 'era', 'pi', 'donde', 'whats', 'habrás', 'hubiste', 'titular', 'habrán', 'f', 'tuviera', 'fuésemos', 'co', 'tendríais', 'mi', 'fui', 'nuestra', 'habré', 'fueron', 'muy', 'nombre', 'tuvierais', 'o', 'erais', '<', 'le', 'bot', 'mía', 'linea', 'hay', 'indica', '#', 'beymar', 'otra', 'tuvieran', 'linar', 're', 'estuvieron', 'han', 'to', 'dire ion', 'aga', 'ttl', 'estés', \"''\", 'ap', 'serías', 'ac', 'cedula', 'cuando', 'tuya', 'está', 'tiene', '{', 'tendré', 'serás', 'me', 'vosotras', 'te', 'ha', 'estará', 'todo', 'queda', 'poco', 'quienes', 'estando', 'ya', 'ti', 'twitter', 'fue', 'seáis', 'estaré', '’', 'por', 'tendría', 'sentid', 'habrías', \"'\", 'noviembre', 'pineda', 'hubimos', 'he', 'sentida', 'pqr', 'contra', 'dir', 'ella', '~', 'rox', 'mío', 'para', 'eres', 'estuviéramos', 'cop', 'hubo', 'elmer', 'hubieras', 'míos', 'razón', 'aguirre', 'direccion', 'porque', 'el', 'tuviésemos', 'ese', 'serían', 'febrero', 'tel', 'entre', '\"', 'estaba', 'lo', 'rf', 'y', 'teníais', 'hemos', 'dg', 'estuvisteis', 'preferirnos', 'ante', 'tuvieses', 'hayan', 'sr', '$', 'sentidos', 'julio', '”', 'seré', 'estoy', 'les', ']', 'un', 'or', 'estaremos', '+', 'lt', 'nuestro', 'eras', 'septiembre', '=', 'hayamos', 'encuentra', 'hubisteis', 'tuve', 'whast p']\n"
          ]
        }
      ],
      "source": [
        "from lemastem import data_preprocessing\n",
        "\n",
        "\n",
        "clean_descripcion=data_preprocessing(corrected_sentences1)\n",
        "\n",
        "\n",
        "clean_descripcion[883]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['solicitar', 'informacion', 'factura']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_descripcion[109]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# Specify the file path where you want to save the list\n",
        "file_path = \"/content/PRIORIZAR/data/entrenamiento_binario.pkl\"\n",
        "\n",
        "# Open the file in binary write mode\n",
        "with open(file_path, \"wb\") as file:\n",
        "    # Use the pickle.dump() function to save the list to the file\n",
        "    pickle.dump(clean_descripcion, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the file path from where you want to load the list\n",
        "file_path = \"/content/PRIORIZAR/data/entrenamiento_binario.pkl\"\n",
        "\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, \"rb\") as file:\n",
        "    # Use the pickle.load() function to load the list from the file\n",
        "    loaded_list = pickle.load(file)\n",
        "\n",
        "print(loaded_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "clean_descripcion=loaded_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(clean_descripcion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_descripcion[5668]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creacion tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pow0vAOrTlgk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "\n",
        "\n",
        "\n",
        "merged_data=[]\n",
        "for i in range(len(clean_descripcion)):\n",
        "    merged_data.append(\" \".join(clean_descripcion[i]))\n",
        "\n",
        "merged_df=pd.DataFrame(merged_data,columns =['descripcion']).reset_index()\n",
        "\n",
        "#############################################\n",
        "\n",
        "pandas_c_F.reset_index(inplace=True)\n",
        "\n",
        "#merged_df = pd.merge(merged_df, pandas_c_F, on='index', how='inner').loc[:, ['FECHA_CREACION','NUMERO_PQR','descripcion', 'clasificacion']]\n",
        "\n",
        "merged_df = pd.concat([merged_df, pandas_c_F], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#shuffled_df.rename(columns={\"FECHA_CREACION\": \"event_timestamp\"}, inplace=True)\n",
        "\n",
        "#string_values = ['abcde', 'fghij', 'klmno', 'pqrst', 'uvwxy', 'zabcd', 'efghi', 'jklmn', 'opqrs', 'tuvwx']\n",
        "\n",
        "\n",
        "\n",
        "# Función para eliminar las tildes de un texto\n",
        "def remove_accents(text):\n",
        "    return unidecode(text)\n",
        "\n",
        "# Aplicar la función a la columna 'Texto'\n",
        "shuffled_df['descripcion'] = shuffled_df['descripcion'].apply(remove_accents)\n",
        "\n",
        "\n",
        "\n",
        "# Randomly select string values to fill the DataFrame\n",
        "#random_strings = np.random.choice(string_values, size=len(shuffled_df))\n",
        "\n",
        "#shuffled_df[\"NUMERO_PQR\"]=random_strings\n",
        "\n",
        "\n",
        "shuffled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_df.to_csv('/content/PRIORIZAR/entranamiento_/entrana_bina.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_df = pd.read_csv('/content/PRIORIZAR/entranamiento_/entrana_bina.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tfidf import TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "\n",
        "X = shuffled_df['descripcion']\n",
        "\n",
        "y = shuffled_df['clasificacion']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,X_test)\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Get the explained variance ratio for each component\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Find the number of components that explain a desired amount of variance\n",
        "desired_variance = 0.9  # Set the desired variance threshold\n",
        "n_components = np.argmax(cumulative_variance_ratio >= desired_variance) + 1\n",
        "\n",
        "# Create a new instance of PCA with the optimal number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)\n",
        "\n",
        "\n",
        "dump(value=pca, filename=\"/content/PRIORIZAR/modelo/pca_binario.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Objective function to be minimized.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 200),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-2, 5.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-2, 5.0, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 90),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.04, 0.1),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    gbm = lgb.LGBMClassifier(**param)\n",
        "    gbm.fit(X_train_new, y_train)\n",
        "    preds = gbm.predict(X_test_new)\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    return accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampler = TPESampler(seed=1)\n",
        "study = optuna.create_study(study_name=\"lightgbm\", direction=\"maximize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Best parameters:', study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mode = lgb.LGBMClassifier(**study.best_params)\n",
        "mode.fit(X_train_new, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = mode.predict(X_test_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import dump\n",
        "dump(value=mode, filename=\"/content/PRIORIZAR/modelo/modelo_binario.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "para data a predecir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "modelo creacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpKLHItfhZbG"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "X = shuffled_df['descripcion']\n",
        "\n",
        "y = shuffled_df['clasificacion'].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
        "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
        "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
        "    X_test = vectorizer_x.transform(X_test).toarray()\n",
        "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
        "    return (X_train, X_test)\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,X_test)\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=5000)\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH4dFMG3j3ZE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "\n",
        "\n",
        "lr.fit(X_train_new, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# generate predictions\n",
        "\n",
        "predictions = lr.predict(X_test_new)\n",
        "\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from joblib import dump\n",
        "\n",
        "train_data = lgb.Dataset(X_train_new, label=y_train)\n",
        "\n",
        "# Set hyperparameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "model = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_new)\n",
        "\n",
        "y_pred_binary = [1 if val >= 0.5 else 0 for val in y_pred]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_binary), index=['OK','WRONG'], columns=['OK','WRONG'])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dump(value=model, filename=\"model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "predicciendo data nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store = FeatureStore(repo_path=\"/my_project/feature_repo/\")\n",
        "\n",
        "t_predict = store.get_saved_dataset(name=\"prediccion_2\").to_df()\n",
        "\n",
        "t_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = merged_df['descripcion']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
        "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
        "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
        "    X_test = vectorizer_x.transform(X_test).toarray()\n",
        "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
        "    return (X_train, X_test)\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=5000)\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_train_new)\n",
        "\n",
        "y_pred_binary = [1 if val >= 0.5 else 0 for val in y_pred]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pandas_c_F['prediccion_algoritmo'] = y_pred_binary\n",
        "\n",
        "pandas_c_F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_c_F.to_csv('c_facturados.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEhiySj1LlMN"
      },
      "outputs": [],
      "source": [
        "from feast import FeatureStore\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Getting our FeatureStore\n",
        "store = FeatureStore(repo_path=\"/my_project/feature_repo/\")\n",
        "\n",
        "# Code for loading features to online store between two dates\n",
        "\"\"\"store.materialize(\n",
        "    end_date=datetime.now(),\n",
        "    start_date=datetime.now() - timedelta(days=900))\"\"\"\n",
        "\n",
        "# Loading the latest features after a previous materialize call or from the beginning of time\n",
        "#store.materialize_incremental(end_date=datetime.now())\n",
        "\n",
        "\n",
        "store.materialize(\n",
        "    end_date=datetime.now(),\n",
        "    start_date=partida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "\n",
        "partida=datetime.now() - timedelta(days=60)\n",
        "\n",
        "# Print the future date\n",
        "print(f\"Current Date: {partida}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM9gHByaPrQI"
      },
      "outputs": [],
      "source": [
        "feast_features = [\n",
        "        \"prediccion:descripcion\"\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = store.get_online_features(\n",
        "    features=feast_features,    \n",
        "    entity_rows=[{\"NUMERO_PQR\": \"abcde\"}]\n",
        ").to_dict()\n",
        "\n",
        "string_values = ['abcde', 'fghij', 'klmno', 'pqrst', 'uvwxy', 'zabcd', 'efghi', 'jklmn', 'opqrs', 'tuvwx']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame.from_dict(data=features)\n",
        "\n",
        "features_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
