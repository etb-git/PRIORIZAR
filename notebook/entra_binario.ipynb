{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asfRC48B3cr8"
      },
      "source": [
        "En este notebook se hara el proceso de **PRIORIZAR** , se tomaran las PQR Desde diciembre del 2022 a  **2023 a Marzo(periodo de tres meses)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFjCIGvRLWHN"
      },
      "source": [
        "# CONFIGURAR SPARK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.append('../librerias')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSySf-J3Oiy"
      },
      "source": [
        "# LIBRERIAS\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v3kkbvVU9pJ"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "!python -m spacy download es_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HQpIbqTHV86",
        "outputId": "21ef21bc-f152-438d-a8bb-986ebedcc8c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "import gc\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "import os\n",
        "import re, string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "#from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re, string\n",
        "\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer, util\n",
        "#from transformers import *\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm', disable=['tagger', 'parser', 'ner'])\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "#.master(\"local[2]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pycorrector\n",
        "%pip install unidecode\n",
        "%pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26MilXbg3TbA"
      },
      "source": [
        "# ETL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf-CJREU8cSq"
      },
      "outputs": [],
      "source": [
        "\n",
        "pandas_c_F= pd.read_csv('/content/PRIORIZAR/entranamiento_/binario_entrenamiento_cruda.csv',delimiter=';')\n",
        "\n",
        "pandas_c_F= pandas_c_F.fillna(\"vacios\")\n",
        "\n",
        "\n",
        "pandas_c_F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_c_F.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from clean_text import clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#declare answers and questions\n",
        "descripcion=pandas_c_F.DESCRIPCION_PQR\t\n",
        "\n",
        "\n",
        "\n",
        "# Cleaning the questions\n",
        "\n",
        "clean_descripcion = []\n",
        "for des in descripcion:\n",
        "    clean_descripcion.append(clean_text(des))\n",
        "\n",
        "\n",
        "#restar dos para optener la data de excel y poder comparar\n",
        "\n",
        "clean_descripcion  [1002]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(clean_descripcion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from spelling import correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corrected_sentences = []\n",
        "for text in tqdm(clean_descripcion, desc=\"Processing\"):\n",
        "    corrected_sentence = correct_sentence(text)\n",
        "    corrected_sentences.append(corrected_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corrected_sentences1=corrected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11659\n"
          ]
        }
      ],
      "source": [
        "from lemastem import data_preprocessing\n",
        "\n",
        "\n",
        "clean_descripcion=data_preprocessing(corrected_sentences1)\n",
        "\n",
        "\n",
        "clean_descripcion[883]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_descripcion[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# Specify the file path where you want to save the list\n",
        "file_path = \"/content/PRIORIZAR/data/entrenamiento_binario.pkl\"\n",
        "\n",
        "# Open the file in binary write mode\n",
        "with open(file_path, \"wb\") as file:\n",
        "    # Use the pickle.dump() function to save the list to the file\n",
        "    pickle.dump(clean_descripcion, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the file path from where you want to load the list\n",
        "file_path = \"/content/PRIORIZAR/entranamiento_/entrenamiento_binario_final.pkl\"\n",
        "\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, \"rb\") as file:\n",
        "    # Use the pickle.load() function to load the list from the file\n",
        "    loaded_list = pickle.load(file)\n",
        "\n",
        "print(loaded_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "clean_descripcion=loaded_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(clean_descripcion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_descripcion[5668]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creacion tabla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pow0vAOrTlgk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "\n",
        "\n",
        "\n",
        "merged_data=[]\n",
        "for i in range(len(clean_descripcion)):\n",
        "    merged_data.append(\" \".join(clean_descripcion[i]))\n",
        "\n",
        "merged_df=pd.DataFrame(merged_data,columns =['descripcion']).reset_index()\n",
        "\n",
        "#############################################\n",
        "\n",
        "pandas_c_F.reset_index(inplace=True)\n",
        "\n",
        "#merged_df = pd.merge(merged_df, pandas_c_F, on='index', how='inner').loc[:, ['FECHA_CREACION','NUMERO_PQR','descripcion', 'clasificacion']]\n",
        "\n",
        "merged_df = pd.concat([merged_df, pandas_c_F], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "shuffled_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#shuffled_df.rename(columns={\"FECHA_CREACION\": \"event_timestamp\"}, inplace=True)\n",
        "\n",
        "#string_values = ['abcde', 'fghij', 'klmno', 'pqrst', 'uvwxy', 'zabcd', 'efghi', 'jklmn', 'opqrs', 'tuvwx']\n",
        "\n",
        "\n",
        "\n",
        "# Función para eliminar las tildes de un texto\n",
        "def remove_accents(text):\n",
        "    return unidecode(text)\n",
        "\n",
        "# Aplicar la función a la columna 'Texto'\n",
        "shuffled_df['descripcion'] = shuffled_df['descripcion'].apply(remove_accents)\n",
        "\n",
        "\n",
        "\n",
        "# Randomly select string values to fill the DataFrame\n",
        "#random_strings = np.random.choice(string_values, size=len(shuffled_df))\n",
        "\n",
        "#shuffled_df[\"NUMERO_PQR\"]=random_strings\n",
        "\n",
        "\n",
        "shuffled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_df.to_csv('/content/PRIORIZAR/entranamiento_/entrana_bina.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_df = pd.read_csv('/content/PRIORIZAR/entranamiento_/entrana_bina.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tfidf import TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "\n",
        "X = shuffled_df['descripcion']\n",
        "\n",
        "y = shuffled_df['clasificacion']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,X_test)\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Get the explained variance ratio for each component\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Find the number of components that explain a desired amount of variance\n",
        "desired_variance = 0.9  # Set the desired variance threshold\n",
        "n_components = np.argmax(cumulative_variance_ratio >= desired_variance) + 1\n",
        "\n",
        "# Create a new instance of PCA with the optimal number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)\n",
        "\n",
        "\n",
        "dump(value=pca, filename=\"/content/PRIORIZAR/modelo/pca_binario.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Objective function to be minimized.\n",
        "    \"\"\"\n",
        "    param = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 200),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-2, 5.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-2, 5.0, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 90),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.04, 0.1),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    gbm = lgb.LGBMClassifier(**param)\n",
        "    gbm.fit(X_train_new, y_train)\n",
        "    preds = gbm.predict(X_test_new)\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    return accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampler = TPESampler(seed=1)\n",
        "study = optuna.create_study(study_name=\"lightgbm\", direction=\"maximize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Best parameters:', study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mode = lgb.LGBMClassifier(**study.best_params)\n",
        "mode.fit(X_train_new, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = mode.predict(X_test_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import dump\n",
        "dump(value=mode, filename=\"/content/PRIORIZAR/modelo/modelo_binario.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "para data a predecir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "modelo creacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpKLHItfhZbG"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "X = shuffled_df['descripcion']\n",
        "\n",
        "y = shuffled_df['clasificacion'].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
        "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
        "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
        "    X_test = vectorizer_x.transform(X_test).toarray()\n",
        "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
        "    return (X_train, X_test)\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,X_test)\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=5000)\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH4dFMG3j3ZE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "\n",
        "\n",
        "lr.fit(X_train_new, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# generate predictions\n",
        "\n",
        "predictions = lr.predict(X_test_new)\n",
        "\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from joblib import dump\n",
        "\n",
        "train_data = lgb.Dataset(X_train_new, label=y_train)\n",
        "\n",
        "# Set hyperparameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "model = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_new)\n",
        "\n",
        "y_pred_binary = [1 if val >= 0.5 else 0 for val in y_pred]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_binary), index=['OK','WRONG'], columns=['OK','WRONG'])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dump(value=model, filename=\"model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "predicciendo data nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store = FeatureStore(repo_path=\"/my_project/feature_repo/\")\n",
        "\n",
        "t_predict = store.get_saved_dataset(name=\"prediccion_2\").to_df()\n",
        "\n",
        "t_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = merged_df['descripcion']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
        "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
        "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
        "    X_test = vectorizer_x.transform(X_test).toarray()\n",
        "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
        "    return (X_train, X_test)\n",
        "\n",
        "\n",
        "X_train,X_test = TFIDF(X_train,_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=5000)\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\n",
        "\n",
        "print(\"train with old features: \",np.array(X_train).shape)\n",
        "print(\"train with new features:\" ,np.array(X_train_new).shape)\n",
        "\n",
        "print(\"test with old features: \",np.array(X_test).shape)\n",
        "print(\"test with new features:\" ,np.array(X_test_new).shape)\n",
        "\n",
        "\n",
        "\n",
        "print('Training Data :', X_train.shape)\n",
        "\n",
        "print('Testing Data : ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_train_new)\n",
        "\n",
        "y_pred_binary = [1 if val >= 0.5 else 0 for val in y_pred]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pandas_c_F['prediccion_algoritmo'] = y_pred_binary\n",
        "\n",
        "pandas_c_F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_c_F.to_csv('c_facturados.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEhiySj1LlMN"
      },
      "outputs": [],
      "source": [
        "from feast import FeatureStore\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Getting our FeatureStore\n",
        "store = FeatureStore(repo_path=\"/my_project/feature_repo/\")\n",
        "\n",
        "# Code for loading features to online store between two dates\n",
        "\"\"\"store.materialize(\n",
        "    end_date=datetime.now(),\n",
        "    start_date=datetime.now() - timedelta(days=900))\"\"\"\n",
        "\n",
        "# Loading the latest features after a previous materialize call or from the beginning of time\n",
        "#store.materialize_incremental(end_date=datetime.now())\n",
        "\n",
        "\n",
        "store.materialize(\n",
        "    end_date=datetime.now(),\n",
        "    start_date=partida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "\n",
        "partida=datetime.now() - timedelta(days=60)\n",
        "\n",
        "# Print the future date\n",
        "print(f\"Current Date: {partida}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM9gHByaPrQI"
      },
      "outputs": [],
      "source": [
        "feast_features = [\n",
        "        \"prediccion:descripcion\"\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = store.get_online_features(\n",
        "    features=feast_features,    \n",
        "    entity_rows=[{\"NUMERO_PQR\": \"abcde\"}]\n",
        ").to_dict()\n",
        "\n",
        "string_values = ['abcde', 'fghij', 'klmno', 'pqrst', 'uvwxy', 'zabcd', 'efghi', 'jklmn', 'opqrs', 'tuvwx']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame.from_dict(data=features)\n",
        "\n",
        "features_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
